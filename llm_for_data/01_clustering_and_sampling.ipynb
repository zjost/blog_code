{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Download llama-index documentation\n",
    "- Create chunks with markdown parsing\n",
    "- Embed chunks (we used `BAAI/bge-small-en-v1.5`)\n",
    "- Store in LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama-index llama-index-vector-stores-lancedb numpy pandas scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(_vec_store: LanceDBVectorStore) -> pd.DataFrame:\n",
    "    tbl = _vec_store._table\n",
    "    # work-around, see: https://github.com/lancedb/lancedb/issues/2046\n",
    "    df = tbl.head(tbl.count_rows()).to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = LanceDBVectorStore(\n",
    "    uri=\"./data/lancedb\", \n",
    "    mode=\"overwrite\", \n",
    "    query_type=\"hybrid\", \n",
    "    refine_factor=30, \n",
    "    nprobes=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>vector</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30e18eec-8c2e-4737-b640-ef5f24684708</td>\n",
       "      <td>129885b3-f6bb-4fd1-85e9-8af445bb69e3</td>\n",
       "      <td>[-0.045516003, -0.025388127, 0.004847408, -0.0...</td>\n",
       "      <td>::: llama_index.question_gen.openai\\n    optio...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"30e18eec-8c2e-4737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb273b19-15d4-4f3d-994b-1705d450d409</td>\n",
       "      <td>d77cbb09-ee2a-4450-a8ac-19e48a5f8dbe</td>\n",
       "      <td>[-0.079640314, 0.0019300836, 0.018389827, -0.0...</td>\n",
       "      <td>::: llama_index.readers.agent_search\\n    opti...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"eb273b19-15d4-4f3d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>998ba7bc-0623-49e7-b4cb-7df944b9e0bf</td>\n",
       "      <td>742bebe3-2bf4-4321-bb7a-201c54ec4edf</td>\n",
       "      <td>[-0.08911939, -0.012804543, 0.051525857, -0.02...</td>\n",
       "      <td>::: llama_index.readers.airbyte_cdk\\n    optio...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"998ba7bc-0623-49e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3dd102d-4ee9-47cc-ab96-04bc13c3f7a9</td>\n",
       "      <td>21883516-2c9c-4ddc-b15b-3e8551b2378c</td>\n",
       "      <td>[-0.08306652, -0.03686305, 0.05229051, -0.0256...</td>\n",
       "      <td>::: llama_index.readers.airbyte_gong\\n    opti...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"d3dd102d-4ee9-47cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87c36450-ece1-4689-8c78-a391415f9663</td>\n",
       "      <td>d37225d5-9ccb-4cd6-bf6f-186ec85ca24c</td>\n",
       "      <td>[-0.057715178, -0.024075592, 0.03144333, -0.02...</td>\n",
       "      <td>::: llama_index.readers.airbyte_hubspot\\n    o...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"87c36450-ece1-4689...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                                doc_id  \\\n",
       "0  30e18eec-8c2e-4737-b640-ef5f24684708  129885b3-f6bb-4fd1-85e9-8af445bb69e3   \n",
       "1  eb273b19-15d4-4f3d-994b-1705d450d409  d77cbb09-ee2a-4450-a8ac-19e48a5f8dbe   \n",
       "2  998ba7bc-0623-49e7-b4cb-7df944b9e0bf  742bebe3-2bf4-4321-bb7a-201c54ec4edf   \n",
       "3  d3dd102d-4ee9-47cc-ab96-04bc13c3f7a9  21883516-2c9c-4ddc-b15b-3e8551b2378c   \n",
       "4  87c36450-ece1-4689-8c78-a391415f9663  d37225d5-9ccb-4cd6-bf6f-186ec85ca24c   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [-0.045516003, -0.025388127, 0.004847408, -0.0...   \n",
       "1  [-0.079640314, 0.0019300836, 0.018389827, -0.0...   \n",
       "2  [-0.08911939, -0.012804543, 0.051525857, -0.02...   \n",
       "3  [-0.08306652, -0.03686305, 0.05229051, -0.0256...   \n",
       "4  [-0.057715178, -0.024075592, 0.03144333, -0.02...   \n",
       "\n",
       "                                                text  \\\n",
       "0  ::: llama_index.question_gen.openai\\n    optio...   \n",
       "1  ::: llama_index.readers.agent_search\\n    opti...   \n",
       "2  ::: llama_index.readers.airbyte_cdk\\n    optio...   \n",
       "3  ::: llama_index.readers.airbyte_gong\\n    opti...   \n",
       "4  ::: llama_index.readers.airbyte_hubspot\\n    o...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'_node_content': '{\"id_\": \"30e18eec-8c2e-4737...  \n",
       "1  {'_node_content': '{\"id_\": \"eb273b19-15d4-4f3d...  \n",
       "2  {'_node_content': '{\"id_\": \"998ba7bc-0623-49e7...  \n",
       "3  {'_node_content': '{\"id_\": \"d3dd102d-4ee9-47cc...  \n",
       "4  {'_node_content': '{\"id_\": \"87c36450-ece1-4689...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_embeddings(vec_store)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Custom Installation from Pip\n",
      "\n",
      "If you aren't using OpenAI, or want a more selective installation, you can install individual packages as needed.\n",
      "\n",
      "For example, for a local setup with Ollama and HuggingFace embeddings, the installation might look like:\n",
      "\n",
      "```\n",
      "pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
      "```\n",
      "\n",
      "[Check out our Starter Example with Local Models](starter_example_local.md)\n",
      "\n",
      "A full guide to using and configuring LLMs is available [here](../module_guides/models/llms.md).\n",
      "\n",
      "A full guide to using and configuring embedding models is available [here](../module_guides/models/embeddings.md).\n"
     ]
    }
   ],
   "source": [
    "chunk = df.sample(1, random_state=44).text.values[0]\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(df.vector.to_numpy())\n",
    "assert X.shape == (df.shape[0], 384) # change this if you use different embedding size than 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>vector</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30e18eec-8c2e-4737-b640-ef5f24684708</td>\n",
       "      <td>129885b3-f6bb-4fd1-85e9-8af445bb69e3</td>\n",
       "      <td>[-0.045516003, -0.025388127, 0.004847408, -0.0...</td>\n",
       "      <td>::: llama_index.question_gen.openai\\n    optio...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"30e18eec-8c2e-4737...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb273b19-15d4-4f3d-994b-1705d450d409</td>\n",
       "      <td>d77cbb09-ee2a-4450-a8ac-19e48a5f8dbe</td>\n",
       "      <td>[-0.079640314, 0.0019300836, 0.018389827, -0.0...</td>\n",
       "      <td>::: llama_index.readers.agent_search\\n    opti...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"eb273b19-15d4-4f3d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>998ba7bc-0623-49e7-b4cb-7df944b9e0bf</td>\n",
       "      <td>742bebe3-2bf4-4321-bb7a-201c54ec4edf</td>\n",
       "      <td>[-0.08911939, -0.012804543, 0.051525857, -0.02...</td>\n",
       "      <td>::: llama_index.readers.airbyte_cdk\\n    optio...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"998ba7bc-0623-49e7...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d3dd102d-4ee9-47cc-ab96-04bc13c3f7a9</td>\n",
       "      <td>21883516-2c9c-4ddc-b15b-3e8551b2378c</td>\n",
       "      <td>[-0.08306652, -0.03686305, 0.05229051, -0.0256...</td>\n",
       "      <td>::: llama_index.readers.airbyte_gong\\n    opti...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"d3dd102d-4ee9-47cc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87c36450-ece1-4689-8c78-a391415f9663</td>\n",
       "      <td>d37225d5-9ccb-4cd6-bf6f-186ec85ca24c</td>\n",
       "      <td>[-0.057715178, -0.024075592, 0.03144333, -0.02...</td>\n",
       "      <td>::: llama_index.readers.airbyte_hubspot\\n    o...</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"87c36450-ece1-4689...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                                doc_id  \\\n",
       "0  30e18eec-8c2e-4737-b640-ef5f24684708  129885b3-f6bb-4fd1-85e9-8af445bb69e3   \n",
       "1  eb273b19-15d4-4f3d-994b-1705d450d409  d77cbb09-ee2a-4450-a8ac-19e48a5f8dbe   \n",
       "2  998ba7bc-0623-49e7-b4cb-7df944b9e0bf  742bebe3-2bf4-4321-bb7a-201c54ec4edf   \n",
       "3  d3dd102d-4ee9-47cc-ab96-04bc13c3f7a9  21883516-2c9c-4ddc-b15b-3e8551b2378c   \n",
       "4  87c36450-ece1-4689-8c78-a391415f9663  d37225d5-9ccb-4cd6-bf6f-186ec85ca24c   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [-0.045516003, -0.025388127, 0.004847408, -0.0...   \n",
       "1  [-0.079640314, 0.0019300836, 0.018389827, -0.0...   \n",
       "2  [-0.08911939, -0.012804543, 0.051525857, -0.02...   \n",
       "3  [-0.08306652, -0.03686305, 0.05229051, -0.0256...   \n",
       "4  [-0.057715178, -0.024075592, 0.03144333, -0.02...   \n",
       "\n",
       "                                                text  \\\n",
       "0  ::: llama_index.question_gen.openai\\n    optio...   \n",
       "1  ::: llama_index.readers.agent_search\\n    opti...   \n",
       "2  ::: llama_index.readers.airbyte_cdk\\n    optio...   \n",
       "3  ::: llama_index.readers.airbyte_gong\\n    opti...   \n",
       "4  ::: llama_index.readers.airbyte_hubspot\\n    o...   \n",
       "\n",
       "                                            metadata  cluster  \n",
       "0  {'_node_content': '{\"id_\": \"30e18eec-8c2e-4737...        2  \n",
       "1  {'_node_content': '{\"id_\": \"eb273b19-15d4-4f3d...        2  \n",
       "2  {'_node_content': '{\"id_\": \"998ba7bc-0623-49e7...        2  \n",
       "3  {'_node_content': '{\"id_\": \"d3dd102d-4ee9-47cc...        2  \n",
       "4  {'_node_content': '{\"id_\": \"87c36450-ece1-4689...        2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=20)\n",
    "c = kmeans.fit_predict(X)\n",
    "df[\"cluster\"] = c\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "2     238\n",
       "15    198\n",
       "18    136\n",
       "7     131\n",
       "19    121\n",
       "11    100\n",
       "3      88\n",
       "0      67\n",
       "5      66\n",
       "8      59\n",
       "12     56\n",
       "4      55\n",
       "10     53\n",
       "17     38\n",
       "16     36\n",
       "9      32\n",
       "1      30\n",
       "14     29\n",
       "13     22\n",
       "6      17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.express.colors import qualitative\n",
    "from plotly.graph_objs import FigureWidget\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(df, indices, n_samples=10):\n",
    "    for _, row in df.iloc[indices][['metadata', 'text']].head(n_samples).iterrows():\n",
    "        print(f\"\"\"[Filename]: {row.metadata['filename']}\n",
    "[Text]: \n",
    "{row.text[:500]}\n",
    "{\"-\"*50}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tsne = KMeans(n_clusters=20)\n",
    "c_tsne = kmeans_tsne.fit_predict(X_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_tsne\"] = c_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_tsne\n",
       "2     112\n",
       "16    110\n",
       "11    108\n",
       "12    107\n",
       "0      98\n",
       "4      95\n",
       "18     91\n",
       "6      85\n",
       "19     79\n",
       "9      77\n",
       "13     76\n",
       "10     72\n",
       "3      71\n",
       "17     69\n",
       "5      65\n",
       "1      62\n",
       "8      60\n",
       "14     51\n",
       "15     47\n",
       "7      37\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cluster_tsne\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece645d3c855460ea1c545e5dab0084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hovertemplate': ('<b>%{hovertext}</b><br><br>x=%' ... '%{marker.color}<extra></extra>'),\n",
       "              'hovertext': array(['::: llama_index.question_gen.openai\\n    options:\\n ',\n",
       "                                  '::: llama_index.readers.agent_search\\n    options:\\n',\n",
       "                                  '::: llama_index.readers.airbyte_cdk\\n    options:\\n ', ...,\n",
       "                                  '# Querying CSVs\\n\\nTODO', '# Parsing Tables and Charts\\n\\nTODO',\n",
       "                                  '# Text to SQL\\n\\nTODO'], dtype=object),\n",
       "              'legendgroup': '',\n",
       "              'marker': {'color': array([ 2, 11, 12, ...,  2,  0,  2], dtype=int32),\n",
       "                         'coloraxis': 'coloraxis',\n",
       "                         'size': 5,\n",
       "                         'symbol': 'circle'},\n",
       "              'mode': 'markers',\n",
       "              'name': '',\n",
       "              'showlegend': False,\n",
       "              'type': 'scattergl',\n",
       "              'uid': '92b29f44-e937-4b2e-b715-3ddc82b4ff8a',\n",
       "              'x': array([-22.432392 ,   2.0459008, -40.270493 , ..., -15.698424 , -14.108546 ,\n",
       "                          -26.914705 ], dtype=float32),\n",
       "              'xaxis': 'x',\n",
       "              'y': array([ 3.9810047 ,  4.1934185 , -6.665196  , ...,  3.2622929 , -8.382142  ,\n",
       "                          -0.21615873], dtype=float32),\n",
       "              'yaxis': 'y'}],\n",
       "    'layout': {'coloraxis': {'colorbar': {'title': {'text': 'Cluster'}},\n",
       "                             'colorscale': [[0.0, '#0d0887'], [0.1111111111111111,\n",
       "                                            '#46039f'], [0.2222222222222222,\n",
       "                                            '#7201a8'], [0.3333333333333333,\n",
       "                                            '#9c179e'], [0.4444444444444444,\n",
       "                                            '#bd3786'], [0.5555555555555556,\n",
       "                                            '#d8576b'], [0.6666666666666666,\n",
       "                                            '#ed7953'], [0.7777777777777778,\n",
       "                                            '#fb9f3a'], [0.8888888888888888,\n",
       "                                            '#fdca26'], [1.0, '#f0f921']]},\n",
       "               'height': 800,\n",
       "               'legend': {'tracegroupgap': 0},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Chunk Embeddings'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'x'}},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'y'}}}\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a scatter plot with hover annotations\n",
    "annos = df.text.str.slice(0, 50)\n",
    "fig = px.scatter(\n",
    "    x=X_tsne[:, 0], \n",
    "    y=X_tsne[:, 1], \n",
    "    hover_name=annos,\n",
    "    # color=c,\n",
    "    color=c_tsne,\n",
    "    color_discrete_sequence=qualitative.Set1,\n",
    "    labels={\"color\": \"Cluster\"},\n",
    "    title=\"Chunk Embeddings\",\n",
    "    width=800, height=800,\n",
    ")\n",
    "\n",
    "# Enable selection of points\n",
    "fig.update_traces(marker=dict(size=5), selector=dict(mode='markers'))\n",
    "\n",
    "fig_widget = FigureWidget(fig)\n",
    "\n",
    "# Global variable to store selected indices\n",
    "selected_indices = []\n",
    "\n",
    "# Define a callback to capture selected points\n",
    "def on_selection(trace, points, state):\n",
    "    global selected_indices\n",
    "    selected_indices = points.point_inds  # Store selected indices\n",
    "\n",
    "# Attach the callback to the scatter trace\n",
    "scatter_trace = fig_widget.data[0]\n",
    "scatter_trace.on_selection(on_selection)\n",
    "\n",
    "# Display the interactive plot\n",
    "fig_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[selected_indices].cluster_tsne.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(df, selected_indices, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster, sample 10 chunks from each\n",
    "df_samples = df.groupby(\"cluster_tsne\").apply(\n",
    "    lambda x: x.sample(10, random_state=44).index.values,\n",
    "    include_groups=False,\n",
    ")#.reset_index(name='samples')#.explode('samples').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_tsne\n",
       "0    [783, 851, 600, 1432, 746, 1081, 525, 599, 597...\n",
       "1    [1511, 992, 365, 1501, 991, 1188, 990, 1496, 1...\n",
       "2    [103, 170, 324, 264, 175, 80, 21, 311, 1088, 176]\n",
       "3    [1428, 1568, 590, 389, 1426, 1452, 1453, 888, ...\n",
       "4    [822, 834, 804, 1363, 819, 848, 577, 803, 801,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Filename]: index.md\n",
      "[Text]: \n",
      "### Local Cache Management\n",
      "\n",
      "Once you have a pipeline, you may want to store and load the cache.\n",
      "\n",
      "```python\n",
      "# save\n",
      "pipeline.persist(\"./pipeline_storage\")\n",
      "\n",
      "# load and restore state\n",
      "new_pipeline = IngestionPipeline(\n",
      "    transformations=[\n",
      "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
      "        TitleExtractor(),\n",
      "    ],\n",
      ")\n",
      "new_pipeline.load(\"./pipeline_storage\")\n",
      "\n",
      "# will run instantly due to the cache\n",
      "nodes = pipeline.run(documents=[Document.example()])\n",
      "```\n",
      "\n",
      "If the cache becomes too large, you\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: index.md\n",
      "[Text]: \n",
      "### Standalone Usage\n",
      "\n",
      "Node parsers can be used on their own:\n",
      "\n",
      "```python\n",
      "from llama_index.core import Document\n",
      "from llama_index.core.node_parser import SentenceSplitter\n",
      "\n",
      "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
      "\n",
      "nodes = node_parser.get_nodes_from_documents(\n",
      "    [Document(text=\"long text\")], show_progress=False\n",
      ")\n",
      "```\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: usage_documents.md\n",
      "[Text]: \n",
      "#### Customizing Embedding Metadata Text\n",
      "\n",
      "Similar to customing the metadata visible to the LLM, we can also customize the metadata visible to embeddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.\n",
      "\n",
      "```python\n",
      "document.excluded_embed_metadata_keys = [\"file_name\"]\n",
      "```\n",
      "\n",
      "Then, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `Metada\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: loading.md\n",
      "[Text]: \n",
      "### High-Level Transformation API\n",
      "\n",
      "Indexes have a `.from_documents()` method which accepts an array of Document objects and will correctly parse and chunk them up. However, sometimes you will want greater control over how your documents are split up.\n",
      "\n",
      "```python\n",
      "from llama_index.core import VectorStoreIndex\n",
      "\n",
      "vector_index = VectorStoreIndex.from_documents(documents)\n",
      "vector_index.as_query_engine()\n",
      "```\n",
      "\n",
      "Under the hood, this splits your Document into Node objects, which are similar to Documents (they\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: index.md\n",
      "[Text]: \n",
      "## Connecting to Vector Databases\n",
      "\n",
      "When running an ingestion pipeline, you can also chose to automatically insert the resulting nodes into a remote vector store.\n",
      "\n",
      "Then, you can construct an index from that vector store later on.\n",
      "\n",
      "```python\n",
      "from llama_index.core import Document\n",
      "from llama_index.embeddings.openai import OpenAIEmbedding\n",
      "from llama_index.core.node_parser import SentenceSplitter\n",
      "from llama_index.core.extractors import TitleExtractor\n",
      "from llama_index.core.ingestion import IngestionPip\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: modules.md\n",
      "[Text]: \n",
      "### SentenceWindowNodeParser\n",
      "\n",
      "The `SentenceWindowNodeParser` is similar to other node parsers, except that it splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata. Note that this metadata will not be visible to the LLM or embedding model.\n",
      "\n",
      "This is most useful for generating embeddings that have a very specific scope. Then, combined with a `MetadataReplacementNodePostProcessor`, you can replace the \n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: rag_cli.md\n",
      "[Text]: \n",
      "### Supported File Types\n",
      "\n",
      "Internally, the `rag` CLI tool uses the [SimpleDirectoryReader](../../module_guides/loading/simpledirectoryreader.md) to parse the raw files in your local filesystem into strings.\n",
      "\n",
      "This module has custom readers for a wide variety of file types. Some of those may require that you `pip install` another module that is needed for parsing that particular file type.\n",
      "\n",
      "If a file type is encountered with a file extension that the `SimpleDirectoryReader` does not have a custom r\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: usage_documents.md\n",
      "[Text]: \n",
      "#### Customizing LLM Metadata Text\n",
      "\n",
      "Typically, a document might have many metadata keys, but you might not want all of them visible to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.\n",
      "\n",
      "We can exclude it like so:\n",
      "\n",
      "``\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: usage_documents.md\n",
      "[Text]: \n",
      "### Customizing the id\n",
      "\n",
      "As detailed in the section [Document Management](../../indexing/document_management.md), the `doc_id` is used to enable efficient refreshing of documents in the index. When using the `SimpleDirectoryReader`, you can automatically set the doc `doc_id` to be the full path to each document:\n",
      "\n",
      "```python\n",
      "from llama_index.core import SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\n",
      "print([x.doc_id for x in documents])\n",
      "```\n",
      "\n",
      "You c\n",
      "--------------------------------------------------\n",
      "\n",
      "[Filename]: loading.md\n",
      "[Text]: \n",
      "### Adding Metadata\n",
      "\n",
      "You can also choose to add metadata to your documents and nodes. This can be done either manually or with [automatic metadata extractors](../../module_guides/loading/documents_and_nodes/usage_metadata_extractor.md).\n",
      "\n",
      "Here are guides on 1) [how to customize Documents](../../module_guides/loading/documents_and_nodes/usage_documents.md), and 2) [how to customize Nodes](../../module_guides/loading/documents_and_nodes/usage_nodes.md).\n",
      "\n",
      "```python\n",
      "document = Document(\n",
      "    text=\"tex\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_samples(df, df_samples[4], n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df_samples.explode()].to_parquet(\n",
    "    './data/eval_sampled.parquet'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
