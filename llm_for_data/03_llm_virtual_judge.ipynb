{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM as a Virtual Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ollama import chat\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_virtual_judge_prompt(question: str, answer_true: str, answer_rag: str) -> str:\n",
    "    prompt = f\"\"\"You are an expert evaluator assessing the quality of answers generated by \n",
    "a RAG (Retrieval-Augmented Generation) system. Your task is to compare a generated answer \n",
    "against a reference answer and provide a detailed evaluation.\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Reference Answer: \n",
    "{answer_true}\n",
    "\n",
    "Generated Answer: \n",
    "{answer_rag}\n",
    "\n",
    "Please evaluate the generated answer through the following steps:\n",
    "\n",
    "Step 1: Analyze the factual accuracy by comparing specific claims in the generated answer against \n",
    "the reference answer. List which facts are correct and which (if any) are incorrect or misrepresented.\n",
    "\n",
    "Step 2: Identify any important information from the reference answer that is missing in the \n",
    "generated answer.\n",
    "\n",
    "Step 3: Determine if the generated answer contains any hallucinated information (claims \n",
    "not supported by the reference answer).\n",
    "\n",
    "Step 4: Assess how directly the generated answer addresses the original question.\n",
    "\n",
    "Based on your analysis, provide scores on a scale of 1-5 for each dimension:\n",
    "\n",
    "Factual Accuracy (1-5):\n",
    "1: Contains multiple factual errors\n",
    "3: Contains minor inaccuracies\n",
    "5: All facts are completely accurate\n",
    "\n",
    "Completeness (1-5):\n",
    "1: Missing most key information\n",
    "3: Contains core information but omits some details\n",
    "5: Covers all important information from the reference\n",
    "\n",
    "Relevance (1-5):\n",
    "1: Barely addresses the question\n",
    "3: Addresses the main question but with tangential information\n",
    "5: Directly and specifically answers the question\n",
    "\n",
    "Hallucination (1-5):\n",
    "1: Contains significant made-up information\n",
    "3: Contains minor details not in the reference\n",
    "5: No hallucinated content whatsoever\n",
    "\n",
    "Finally, provide an Overall Score (1-5) that reflects the answer's overall quality, \n",
    "and a brief justification for your evaluation.\n",
    "\n",
    "Overall Score (1-5):\n",
    "Justification:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualJudgeResponse(BaseModel):\n",
    "    factual_accuracy: int = Field(\n",
    "        description=\"Factual Accuracy (1-5)\"\n",
    "    )\n",
    "    completeness: int = Field(\n",
    "        description=\"Completeness (1-5)\"\n",
    "    )\n",
    "    relevance: int = Field(\n",
    "        description=\"Relevance (1-5)\"\n",
    "    )\n",
    "    hallucination: int = Field(\n",
    "        description=\"Hallucination (1-5)\"\n",
    "    )\n",
    "    overall: int = Field(\n",
    "        description=\"Overall Score (1-5)\"\n",
    "    )\n",
    "    justification: str = Field(\n",
    "        description=\"Justification for the evaluation\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answer(\n",
    "    question: str,\n",
    "    answer_true: str,\n",
    "    answer_rag: str,\n",
    "    model: str ='llama3.1:latest'\n",
    ") -> VirtualJudgeResponse:\n",
    "    prompt = generate_virtual_judge_prompt(question, answer_true, answer_rag)\n",
    "    \n",
    "    response = chat(\n",
    "        messages=[\n",
    "            {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        format=VirtualJudgeResponse.model_json_schema(),\n",
    "    )\n",
    "    response_structured = VirtualJudgeResponse.model_validate_json(\n",
    "        response.message.content\n",
    "    )\n",
    "    return response_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator assessing the quality of answers generated by \n",
      "a RAG (Retrieval-Augmented Generation) system. Your task is to compare a generated answer \n",
      "against a reference answer and provide a detailed evaluation.\n",
      "\n",
      "Question: \n",
      "What is the capital of France?\n",
      "\n",
      "Reference Answer: \n",
      "The capital of France is Paris.\n",
      "\n",
      "Generated Answer: \n",
      "Paris is the capital of France.\n",
      "\n",
      "Please evaluate the generated answer through the following steps:\n",
      "\n",
      "Step 1: Analyze the factual accuracy by comparing specific claims in the generated answer against \n",
      "the reference answer. List which facts are correct and which (if any) are incorrect or misrepresented.\n",
      "\n",
      "Step 2: Identify any important information from the reference answer that is missing in the \n",
      "generated answer.\n",
      "\n",
      "Step 3: Determine if the generated answer contains any hallucinated information (claims \n",
      "not supported by the reference answer).\n",
      "\n",
      "Step 4: Assess how directly the generated answer addresses the original question.\n",
      "\n",
      "Based on your analysis, provide scores on a scale of 1-5 for each dimension:\n",
      "\n",
      "Factual Accuracy (1-5):\n",
      "1: Contains multiple factual errors\n",
      "3: Contains minor inaccuracies\n",
      "5: All facts are completely accurate\n",
      "\n",
      "Completeness (1-5):\n",
      "1: Missing most key information\n",
      "3: Contains core information but omits some details\n",
      "5: Covers all important information from the reference\n",
      "\n",
      "Relevance (1-5):\n",
      "1: Barely addresses the question\n",
      "3: Addresses the main question but with tangential information\n",
      "5: Directly and specifically answers the question\n",
      "\n",
      "Hallucination (1-5):\n",
      "1: Contains significant made-up information\n",
      "3: Contains minor details not in the reference\n",
      "5: No hallucinated content whatsoever\n",
      "\n",
      "Finally, provide an Overall Score (1-5) that reflects the answer's overall quality, \n",
      "and a brief justification for your evaluation.\n",
      "\n",
      "Overall Score (1-5):\n",
      "Justification:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ = generate_virtual_judge_prompt(\n",
    "    question=\"What is the capital of France?\",\n",
    "    answer_true=\"The capital of France is Paris.\",\n",
    "    answer_rag=\"Paris is the capital of France.\"\n",
    ")\n",
    "print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_judge_response = judge_answer(\n",
    "    question=\"What is the capital of France?\",\n",
    "    answer_true=\"The capital of France is Paris.\",\n",
    "    answer_rag=\"Paris is the capital of France.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"factual_accuracy\": 5,\n",
      "    \"completeness\": 4,\n",
      "    \"relevance\": 5,\n",
      "    \"hallucination\": 5,\n",
      "    \"overall\": 4,\n",
      "    \"justification\": \"The generated answer is factually accurate, covering all the information required by the reference answer. It may lack a specific detail but does not contain any hallucinated content or significant inaccuracies. The overall quality is good but could be improved to achieve perfection.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(virtual_judge_response.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers from RAG system\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_eval_data(chunk_data_fp: str, eval_data_fp: str) -> pd.DataFrame:\n",
    "\n",
    "    df_chunks = pd.read_parquet(chunk_data_fp)\n",
    "\n",
    "    df_tmp = pd.read_json(eval_data_fp, lines=True)\n",
    "    df_tmp_exploded = df_tmp.explode('qa_pairs')\n",
    "    # expand and add question-answer columns\n",
    "    df_eval = pd.concat([\n",
    "        df_tmp_exploded.drop(columns=['qa_pairs']),\n",
    "        df_tmp_exploded['qa_pairs'].apply(pd.Series),\n",
    "    ], axis=1)\n",
    "    # expand and add metadata columns, and merge original data\n",
    "    df_eval = pd.concat([\n",
    "        df_eval.drop(columns=['metadata']),\n",
    "        df_eval['metadata'].apply(pd.Series),\n",
    "    ], axis=1).merge(\n",
    "        df_chunks[['id', 'doc_id', 'text', 'cluster_tsne', 'metadata']],\n",
    "        on='id',\n",
    "    )\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>required_context</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>q_a_quality</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>cluster_tsne</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>783</td>\n",
       "      <td>871e39f3-ad80-413d-9353-93b39da8adf5</td>\n",
       "      <td>What is the primary function of a data connect...</td>\n",
       "      <td>A data connector (or 'Reader') ingests data fr...</td>\n",
       "      <td>factual</td>\n",
       "      <td>easy</td>\n",
       "      <td>basic understanding of terminology</td>\n",
       "      <td>This question directly asks for the role of a ...</td>\n",
       "      <td>good</td>\n",
       "      <td>aa4c9403-c960-442a-aca3-31ad8ae64f6e</td>\n",
       "      <td>## Concept\\n\\nA data connector (aka `Reader`) ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"871e39f3-ad80-413d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>783</td>\n",
       "      <td>871e39f3-ad80-413d-9353-93b39da8adf5</td>\n",
       "      <td>According to the document, what are the typica...</td>\n",
       "      <td>After data ingestion, you can build an Index, ...</td>\n",
       "      <td>inferential</td>\n",
       "      <td>medium</td>\n",
       "      <td>understanding of workflow</td>\n",
       "      <td>This question requires the user to understand ...</td>\n",
       "      <td>good</td>\n",
       "      <td>aa4c9403-c960-442a-aca3-31ad8ae64f6e</td>\n",
       "      <td>## Concept\\n\\nA data connector (aka `Reader`) ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"871e39f3-ad80-413d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>783</td>\n",
       "      <td>871e39f3-ad80-413d-9353-93b39da8adf5</td>\n",
       "      <td>Explain the overall purpose of using data conn...</td>\n",
       "      <td>The purpose is to take data from various sourc...</td>\n",
       "      <td>analytical</td>\n",
       "      <td>hard</td>\n",
       "      <td>understanding of overall system architecture &amp;...</td>\n",
       "      <td>This requires the user to synthesize the infor...</td>\n",
       "      <td>good</td>\n",
       "      <td>aa4c9403-c960-442a-aca3-31ad8ae64f6e</td>\n",
       "      <td>## Concept\\n\\nA data connector (aka `Reader`) ...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"871e39f3-ad80-413d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>851</td>\n",
       "      <td>45e0ab38-6280-4862-be9f-b57ce7f96492</td>\n",
       "      <td>What is the primary subject matter described i...</td>\n",
       "      <td>Relation-Based Node Parsers</td>\n",
       "      <td>factual</td>\n",
       "      <td>easy</td>\n",
       "      <td>entire chunk</td>\n",
       "      <td>This question tests basic recall - directly as...</td>\n",
       "      <td>good</td>\n",
       "      <td>5f858553-f1ec-4828-88df-b6dce5754a75</td>\n",
       "      <td>## Relation-Based Node Parsers</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"45e0ab38-6280-4862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851</td>\n",
       "      <td>45e0ab38-6280-4862-be9f-b57ce7f96492</td>\n",
       "      <td>Based on the title, what kind of parsing are t...</td>\n",
       "      <td>Parsing that involves relationships between no...</td>\n",
       "      <td>inferential</td>\n",
       "      <td>medium</td>\n",
       "      <td>title only</td>\n",
       "      <td>This requires inference – the user has to unde...</td>\n",
       "      <td>good</td>\n",
       "      <td>5f858553-f1ec-4828-88df-b6dce5754a75</td>\n",
       "      <td>## Relation-Based Node Parsers</td>\n",
       "      <td>0</td>\n",
       "      <td>{'_node_content': '{\"id_\": \"45e0ab38-6280-4862...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                    id  \\\n",
       "0  783  871e39f3-ad80-413d-9353-93b39da8adf5   \n",
       "1  783  871e39f3-ad80-413d-9353-93b39da8adf5   \n",
       "2  783  871e39f3-ad80-413d-9353-93b39da8adf5   \n",
       "3  851  45e0ab38-6280-4862-be9f-b57ce7f96492   \n",
       "4  851  45e0ab38-6280-4862-be9f-b57ce7f96492   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the primary function of a data connect...   \n",
       "1  According to the document, what are the typica...   \n",
       "2  Explain the overall purpose of using data conn...   \n",
       "3  What is the primary subject matter described i...   \n",
       "4  Based on the title, what kind of parsing are t...   \n",
       "\n",
       "                                              answer question_type difficulty  \\\n",
       "0  A data connector (or 'Reader') ingests data fr...       factual       easy   \n",
       "1  After data ingestion, you can build an Index, ...   inferential     medium   \n",
       "2  The purpose is to take data from various sourc...    analytical       hard   \n",
       "3                        Relation-Based Node Parsers       factual       easy   \n",
       "4  Parsing that involves relationships between no...   inferential     medium   \n",
       "\n",
       "                                    required_context  \\\n",
       "0                 basic understanding of terminology   \n",
       "1                          understanding of workflow   \n",
       "2  understanding of overall system architecture &...   \n",
       "3                                       entire chunk   \n",
       "4                                         title only   \n",
       "\n",
       "                                           reasoning q_a_quality  \\\n",
       "0  This question directly asks for the role of a ...        good   \n",
       "1  This question requires the user to understand ...        good   \n",
       "2  This requires the user to synthesize the infor...        good   \n",
       "3  This question tests basic recall - directly as...        good   \n",
       "4  This requires inference – the user has to unde...        good   \n",
       "\n",
       "                                 doc_id  \\\n",
       "0  aa4c9403-c960-442a-aca3-31ad8ae64f6e   \n",
       "1  aa4c9403-c960-442a-aca3-31ad8ae64f6e   \n",
       "2  aa4c9403-c960-442a-aca3-31ad8ae64f6e   \n",
       "3  5f858553-f1ec-4828-88df-b6dce5754a75   \n",
       "4  5f858553-f1ec-4828-88df-b6dce5754a75   \n",
       "\n",
       "                                                text  cluster_tsne  \\\n",
       "0  ## Concept\\n\\nA data connector (aka `Reader`) ...             0   \n",
       "1  ## Concept\\n\\nA data connector (aka `Reader`) ...             0   \n",
       "2  ## Concept\\n\\nA data connector (aka `Reader`) ...             0   \n",
       "3                     ## Relation-Based Node Parsers             0   \n",
       "4                     ## Relation-Based Node Parsers             0   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'_node_content': '{\"id_\": \"871e39f3-ad80-413d...  \n",
       "1  {'_node_content': '{\"id_\": \"871e39f3-ad80-413d...  \n",
       "2  {'_node_content': '{\"id_\": \"871e39f3-ad80-413d...  \n",
       "3  {'_node_content': '{\"id_\": \"45e0ab38-6280-4862...  \n",
       "4  {'_node_content': '{\"id_\": \"45e0ab38-6280-4862...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_filepath = './data/eval_sampled.parquet'\n",
    "eval_filepath = './data/qa_pairs_gemma.jsonl'\n",
    "df_eval = load_and_prepare_eval_data(chunk_filepath, eval_filepath)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGE_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "BGE_SMALL_QUERY_INSTRUCTION = \"Represent this sentence for searching relevant passages:\"\n",
    "\n",
    "model = HuggingFaceEmbedding(\n",
    "    model_name=BGE_MODEL_NAME,\n",
    "    query_instruction=BGE_SMALL_QUERY_INSTRUCTION,\n",
    "    device=\"cuda\",\n",
    "    cache_folder=\"/home/zak/git/local_rag_course/local_rag/models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = LanceDBVectorStore(\n",
    "    uri=\"/home/zak/git/local_rag_course/local_rag/data/lancedb\", \n",
    "    mode=\"overwrite\", \n",
    "    query_type=\"vector\", \n",
    "    refine_factor=30, \n",
    "    nprobes=100,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vec_store, embed_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model='llama3.1:latest', \n",
    "    request_timeout=120.0,\n",
    "    temperature=0.75,\n",
    ")\n",
    "Settings.llm = llm\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, a_true = df_eval.iloc[0][['question', 'answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"What is the primary function of a data connector (or 'Reader') within this system?\",\n",
       " \"A data connector (or 'Reader') ingests data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, a_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response = query_engine.query(q)\n",
    "a_rag = rag_response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A data connector (often called a `Reader`) ingests data from different data sources and data formats into `Documents` and `Nodes`.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"factual_accuracy\": 4,\n",
      "    \"completeness\": 3,\n",
      "    \"relevance\": 5,\n",
      "    \"hallucination\": 5,\n",
      "    \"overall\": 4,\n",
      "    \"justification\": \"The generated answer is almost entirely accurate and complete, directly addressing the question. However, it misses some minor details from the reference.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "virtual_judge_response = judge_answer(\n",
    "    question=q,\n",
    "    answer_true=a_true,\n",
    "    answer_rag=a_rag,\n",
    "    model='llama3.1:latest'\n",
    ")\n",
    "print(json.dumps(virtual_judge_response.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG answer quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
